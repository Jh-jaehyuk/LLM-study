# Week 2

> ğŸ’¡**ìš”ì•½** : **Transformer Encoder**

## Recap
* **RNNì˜ ë‹¨ì **
1. Long-Term Dependencies
2. Recurrent ì—°ì‚°ì´ ë„ˆë¬´ ë¬´ê±°ì›€
* **RNNì˜ ë‹¨ì ì„ ê·¹ë³µí•œ Transformer**  
  â†’ *Transformer = RNN + Attention + Positional Encoding - Recurrent*
* **Attention**  
  â†’ í˜„ì¬ ì‹œì ì˜ ë‹¨ì–´ì™€ ì…ë ¥ì— ë“¤ì–´ì˜¨ ëª¨ë“  ë‹¨ì–´ ì‚¬ì´ì˜ ì—°ê´€ë„ë¥¼ ë‚´ì ì„ ì´ìš©í•˜ì—¬ íŒë‹¨
* **Positional Encoding**  
  â†’ ì…ë ¥ì˜ ì„ í›„ ê´€ê³„(ìˆœì„œ)ë¥¼ ì•Œë ¤ì£¼ëŠ” ë²¡í„°
  
---
## Transformer
![Transformer structure](https://github.com/user-attachments/assets/2389f7c5-806a-4555-9e5d-f68a4cca3df2)  

### Encoder
* **Self-Attention**  
![Self attention-1](https://github.com/user-attachments/assets/eecb2b2d-1b3a-4a18-8aac-ff20b5fce23e)  
![Self attention-2](https://github.com/user-attachments/assets/6c18de49-985e-4e36-8f65-fabea9ff8e0c)  

  **Q(Query)**: ì…ë ¥ì—ì„œ ê´€ë ¨ëœ ë¶€ë¶„ì„ ì°¾ìœ¼ë ¤ê³  í•˜ëŠ” ì •ë³´ ë²¡í„°  
  **K(Key)**: ê´€ê³„ì˜ ì—°ê´€ë„ë¥¼ ê²°ì •í•˜ê¸° ìœ„í•´ Qì™€ ë¹„êµí•˜ëŠ”ë° ì‚¬ìš©ë˜ëŠ” ë²¡í„°  
  **V(Value)**: íŠ¹ì • Kì— í•´ë‹¹í•˜ëŠ” ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ê°€ì ¸ì˜¬ì§€ì— ëŒ€í•œ í‘œí˜„
  ![Self attention-3](https://github.com/user-attachments/assets/9896f527-ecaf-4c78-ab42-52cd3e466269)
    
  
* **Multi-Head Self-Attention(MHA)**  
ğŸ”ì´ ì „ ê³¼ì •ì€ Headê°€ í•˜ë‚˜ì¼ ë•Œì˜ Self-Attention    
MHAëŠ” _Headë¥¼ ì—¬ëŸ¬ ê°œ_ ë‘ì–´ ê° Headë§ˆë‹¤ Attentionì„ ìˆ˜í–‰í•¨!  
â­ï¸Headê°€ í•˜ë‚˜ì¸ ê²½ìš°ë³´ë‹¤ **ë” ì¢‹ì€ ì„±ëŠ¥**ì„ ê°–ì„ ìˆ˜ ìˆë‹¤.  
![Multi Head Self attention-1](https://github.com/user-attachments/assets/be6cf61d-515a-46a4-a56a-d417dfeac1ac)
  
  
* **Add & Layer Normalization(LN)**
  > CNNì—ì„œ Feature mapì— ëŒ€í•œ Normalizationì„ ìœ„í•´ 
  > Batch Normalizationì„ ì ìš©í–ˆë“¯ì´, Transformerì—ì„œëŠ”
  > Layer Normalizationì„ ìˆ˜í–‰í•¨

  **ğŸ¤”Normalization ì™œ ì”€?**
  1. **í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ**  
  2. **ë” ë¹ ë¥¸ í•™ìŠµ ì†ë„**  
  3. **ê³¼ì í•© ë°©ì§€**  
  
  â†’ Normalizationì€ ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê³  í•™ìŠµì„ ë³´ë‹¤ íš¨ìœ¨ì ì´ê³  ì•ˆì •ì ìœ¼ë¡œ ë§Œë“¤ê¸° ìœ„í•´ í•„ìˆ˜ì ì¸ ê³¼ì •

  ![Add and LN-1](https://github.com/user-attachments/assets/b882a014-331f-4fab-9ac4-54287b713740)  
  ![Add and LN-2](https://github.com/user-attachments/assets/5c88cb49-3642-41d1-a1ed-7d3aae72e24c)
  ![Add and LN-3](https://github.com/user-attachments/assets/3d5f2cf9-4395-43f9-a4db-d637c92ce7d6)

---
### ê²° ë¡ 
![Result-1](https://github.com/user-attachments/assets/8d1a12ea-55f2-48c8-ab71-b2a73a03d73e)

* **Self-Attention**  
ì…ë ¥ ë¬¸ì¥(ìì‹ )ì— ëŒ€í•´ Query, Keyë¡œ ë³´ë‚´ Attention ì—°ì‚°ì„ í•˜ë¯€ë¡œ Self-Attentionì´ë¼ ë¶€ë¦„!  
Self-Attentionì„ í†µí•´ì„œ ê°™ì€ ë¬¸ì¥ ë‚´ ëª¨ë“  ë‹¨ì–´ ì‚¬ì´ì˜ ì˜ë¯¸ì , ë¬¸ë²•ì  ê´€ê³„ë¥¼ í¬ì°©
  

* **Multi-Head Self-Attention(MHA)**  
ë™ì¼í•œ ë¬¸ì¥ì— ëŒ€í•´ ì—¬ëŸ¬ ëª…ì˜ ë…ì(head)ê°€ ê´€ê³„ë¥¼ ë¶„ì„  
í˜¼ì í–ˆì„ ë•Œ ë³´ë‹¨ ì¢€ ë” ì¼ë°˜í™”ë˜ê³  ì¢‹ì€ ê´€ê³„ë¥¼ ë¶„ì„ ê°€ëŠ¥!
  

* **Add & Layer Normalization**  
_Add_ : layerê°€ ê¹Šì–´ì§€ë”ë¼ë„ í•™ìŠµì„ ìš©ì´í•˜ê²Œ ë§Œë“¦ (ì´ì „ì˜ ì •ë³´ë¥¼ ìŠì§€ ì•ŠëŠ”ë‹¤.)  
_Layer Normalization_ : CNNì— ë¹„í•´ ìƒëŒ€ì ìœ¼ë¡œ ë¬´ê±°ìš´ ëª¨ë¸ì¸ Transformer êµ¬ì¡°ì—ì„œ BN ë°©ì‹ë³´ë‹¤ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµ ê°€ëŠ¥