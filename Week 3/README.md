# Week 3

> ğŸ’¡**ìš”ì•½** : **Transformer Decoder**

## Recap
* **Self-Attention**  
ì…ë ¥ ë¬¸ì¥(ìì‹ )ì— ëŒ€í•´ Query, Keyë¥¼ ë³´ë‚´ Attention ì—°ì‚°ì„ í•˜ë¯€ë¡œ Self-Attentionì´ë¼ê³  í•¨  
Self-Attentionì„ í†µí•´ì„œ ê°™ì€ ë¬¸ì¥ ë‚´ ëª¨ë“  ë‹¨ì–´ ì‚¬ì´ì˜ ì˜ë¯¸ì , ë¬¸ë²•ì  ê´€ê³„ë¥¼ í¬ì°©  
* **Multi-Head Self-Attention(MHA)**  
ë™ì¼í•œ ë¬¸ì¥ì— ëŒ€í•´ ì—¬ëŸ¬ ëª…ì˜ ì‚¬ëŒ(Head)ê°€ ê´€ê³„ë¥¼ ë¶„ì„í•˜ëŠ” ê²ƒê³¼ ìœ ì‚¬í•¨  
í•œ ëª…ì˜ ì‚¬ëŒì´ ë¶„ì„í•˜ëŠ” ê²ƒë³´ë‹¤ ë” ì¼ë°˜í™”ë˜ê³  ì¢‹ì€ ì—°ê´€ ê´€ê³„ë¥¼ ë¶„ì„ ê°€ëŠ¥  
* **Add & Layer Normalization**  
  * Add: Layerê°€ ê¹Šì–´ì§€ë”ë¼ë„ í•™ìŠµì„ ìš©ì´í•˜ê²Œ ë§Œë“¦(ì´ì „ ì •ë³´ë¥¼ ì˜ ìŠì§€ ì•ŠëŠ”ë‹¤ëŠ” ì˜ë¯¸)
  * Layer Normalization: CNNì— ë¹„í•´ ìƒëŒ€ì ìœ¼ë¡œ ë¬´ê±°ìš´ ëª¨ë¸ì¸ Transformer êµ¬ì¡°ì—ì„œ   
Batch Normalization ë°©ì‹ë³´ë‹¤ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµ ê°€ëŠ¥
* **Feed Forward Netword(FFN)**  
Feed Forwardì—ì„œëŠ” ë‹¨ì–´ ê·¸ ìì²´ì˜ ì˜ë¯¸ë¥¼ íŒŒì•…í•˜ëŠ”ë° ì¤‘ì ì„ ë‘ 
  
---
## Transformer
![Transformer structure](https://github.com/user-attachments/assets/2389f7c5-806a-4555-9e5d-f68a4cca3df2)  

### Encoder vs. Decoder
> ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì—ëŠ” Encoder & Decoder ê°œë…ì´ ë§ì´ ë‚˜ì˜´

1. **Encoder**  
* ë°ì´í„°ì˜ ë‚´ì¬ëœ íŠ¹ì§•ë“¤ì„ ì¶”ì¶œí•˜ëŠ” ê¸°ëŠ¥ì„ ìˆ˜í–‰  
* íŠ¹ì§•ì„ ë§‰ ì¶”ì¶œí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì…ë ¥ ë°ì´í„°ë¥¼ ì˜ í‘œí˜„í•˜ê¸° ìœ„í•œ ì–‘ì§ˆì˜ íŠ¹ì§•ì„ ì¶”ì¶œ  
* ì¶”ì¶œëœ íŠ¹ì§•ë“¤ì€ ê°€ì¤‘ì¹˜ í–‰ë ¬ì— ì €ì¥ë¨  
â†’ **ì»´í“¨í„°ê°€ ë°ì´í„°ë¥¼ ì˜ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í•¨**  

2. **Decoder**  
* Encoderì—ì„œ ì–»ì€ ì–‘ì§ˆì˜ íŠ¹ì§•ì„ í•˜ê³ ì‹¶ì€ ì‘ì—…ì— ì˜ ì ìš©í•  ìˆ˜ ìˆë„ë¡ ë°”ê¿”ì£¼ëŠ” ê¸°ëŠ¥ì„ ìˆ˜í–‰  
â†’ **ì¶”ì¶œëœ ì •ë³´ë¥¼ ì»´í“¨í„°ê°€ ì•„ë‹Œ ì‚¬ëŒì´ ì˜ ì´í•´í•˜ê³  ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•¨**

### Decoder
* **Masked Self-Attention**
```text
í˜„ì¬ ì‹œì ë³´ë‹¤ ë’¤ì— ìˆëŠ” ë‹¨ì–´ë¥¼ ì°¸ì¡°í•˜ì§€ ëª»í•˜ë„ë¡ ê°€ë¦¼(Masking)
```

![Masked Self-Attention1](https://github.com/user-attachments/assets/b7fa94f3-57a0-4fae-93ee-fc11275fd892)  
![Masked Self-Attention2](https://github.com/user-attachments/assets/a14a8159-01ff-46de-b6e2-aa826227192a)
  
* **Encoder-Decoder Attention**
```text
Self-Attentionì´ ìê¸° ìì‹ ê³¼ ë¹„êµí–ˆë‹¤ë©´,
ì—¬ê¸°ëŠ” Encoderë¥¼ í†µí•´ ì¶”ì¶œí•œ ì •ë³´ì™€ Decoderì˜ ì…ë ¥ì„ ì„œë¡œ ë¹„êµ

ë§ ê·¸ëŒ€ë¡œ Encoder ì •ë³´ì™€ Decoder ì…ë ¥ ì‚¬ì´ ìœ ì‚¬ë„ë¥¼ êµ¬í•˜ê¸° ìœ„í•¨
```
![Encoder-Decoder Attention](https://github.com/user-attachments/assets/d7c62e44-8ae0-415c-93ce-e73cdd32a0b3)

* **Output(ì˜ˆì¸¡ ê²°ê³¼)**
```
ì—¬ëŸ¬ ê°œì˜ ë‹¨ì–´ ì¤‘ ì–´ë–¤ ë‹¨ì–´ë¥¼ outputìœ¼ë¡œ ì„ íƒí•  ì§€ ë¶„ë¥˜(Classification)í•˜ëŠ” ê²ƒê³¼ ê°™ìŒ
```
![Output](https://github.com/user-attachments/assets/db0f5082-058f-48ae-9bcb-5f1a043db68c)  
  
### Train vs. Predict
![image](https://github.com/user-attachments/assets/37b264fa-2046-45ad-996e-71b2b07485b5)  
![image](https://github.com/user-attachments/assets/0a2378aa-d655-43d8-9fd3-8c36ff98e7ee)  
![image](https://github.com/user-attachments/assets/ffb01ae3-511a-406f-8a3f-ae0a0a8f01d9)  
![image](https://github.com/user-attachments/assets/a6808b16-08be-49c7-9f9e-2907158d1fb3)  
  
---
### ê²° ë¡ 
* **Masked Self-Attention**  
ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ”ë°, ë’¤ì˜ ë‹¨ì–´ë¥¼ ë¯¸ë¦¬ ì•Œê²Œë˜ëŠ” ê²ƒì€ ë°˜ì¹™ì´ë¯€ë¡œ   
ë’¤ì˜ ë‚´ìš©ì„ ëª» ë³´ê²Œ ê°€ë¦°(Masking) í›„ Self-Attentionì„ í•˜ëŠ” ê²ƒ  

* **Encoder-Decoder Attention**  
Encoderë¡œë¶€í„° Key, Valueë¥¼ ë°›ê³  Decoderë¡œë¶€í„° Queryë¥¼ ë°›ì•„ Attention ì—°ì‚°í•¨  
ë²ˆì—­ëœ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì„ ìœ„í•¨  

* **Workflow in Traning Process**  
ì…ë ¥ìœ¼ë¡œ ì›ë³¸ ì‹œí€€ìŠ¤ì™€ í•¨ê»˜ í•´ë‹¹ ì‹œí€€ìŠ¤ì— ëŒ€í•œ ì •ë‹µ ë ˆì´ë¸”(ë²ˆì—­ëœ ë¬¸ì¥)ì„ ì œê³µ  
Decoderê°€ ì´ì „ ë‹¨ê³„ì—ì„œ ì˜ˆì¸¡í•œ ë‹¨ì–´ë¥¼ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹ ,  
ì‹¤ì œ ì •ë‹µ ì‹œí€€ìŠ¤ì—ì„œ ì´ì „ ë‹¨ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹  

* **Workflow in Testing Process**  
í…ŒìŠ¤íŠ¸ ë‹¨ê³„ì—ì„œëŠ” ëª¨ë¸ì´ ìƒˆë¡œìš´ ì…ë ¥(í•™ìŠµ ë°ì´í„°ì— ì—†ëŠ” ë¬¸ì¥)ì— ëŒ€í•´ ì˜ˆì¸¡ì„ ìˆ˜í–‰  
DecoderëŠ” ì´ì „ ë‹¨ê³„ì—ì„œ ì˜ˆì¸¡í•œ ë‹¨ì–´ë¥¼ ì‚¬ìš©í•´ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ìƒì„±í•¨
